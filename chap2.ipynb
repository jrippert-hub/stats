{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a00237",
   "metadata": {},
   "source": [
    "--- \n",
    "**Intro**\n",
    "\n",
    "Statistical Learning is conducted to explore and understand data through a variety of tools. These tools may be supervised or unsupervised in nature.\n",
    "\n",
    "Stats learning has gone by many names over the course of its history. Early application date all the way back to the early 1800s when regression was used to predict the orbit of planets. Up until the 1970's most of the methods were linear, non-linear methods were too computationally intensive so the advent of computing helped us scale and apply more effective algorithms. Not to mention the commoditizaiton of computing resources and stats learning packages. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271ccf4e",
   "metadata": {},
   "source": [
    "*!pip install ISLP* run this in terminal to have access to the ISLP datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10491fb0",
   "metadata": {},
   "source": [
    "* What is statistical Learning? \n",
    "    * Is all about exploring the function *f* which describes a relationship between the input variables and outcome variables. \n",
    "* Why Predict *f*? \n",
    "    * Conducting prediction. If we have many Xs and want to predict Y. For example we have patient labs and want to predict likelihood of having a rare disease. No matter how well we try to predict *f*, there will always be a component of irreducible error or error which we cannot get rid of perhaps becuase the is information out there leading to our f value or even random noise which we cannot capture. As such the upper bound in terms or irreducible error and its impact on our predictions isnt known. \n",
    "    * Conducting inference. We want to explore the relationship between X and Y. Which predictors are associated with the response? How much does in increase in advertising budget lead to increased sales?\n",
    "* Parametric vs non Parametric: Parametric assume the form of the function. We are assuming our outcome variable is modelled by the input variable which we train on, the the function is limited into the shape its given. Non parametric approaches dont follow an assumed form and therefor are more flexible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65a5def",
   "metadata": {},
   "source": [
    "---\n",
    "**Assessing Model Accuracy & Best Fit**\n",
    "\n",
    "The MSE formula is $MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$\n",
    "\n",
    "This MSE eval metric is used to judge regression methods and represents the difference between our predicted values and our true values. Squaring the different highlights larger errors (worse predictions). When experimenting with different models, we would pick the model or statistical learning method with the lowest average MSE across the test dataset. \n",
    "\n",
    "**Bias & Variance Tradeoff**\n",
    "* Bias is a type of error that results from our model being too simple, not complex enough to capture the relationships or patterns within our datasets. \n",
    "    * $\\text{Bias}[\\hat{f}(x)] = E[\\hat{f}(x)] - f(x)$\n",
    "    * difference between avg pred or expected value and the true value. Bias occurs as we try to model real world phenomena using models which assume a form.\n",
    "* Variance is the flipside. It is the error alikened to overfitting when we introduce too much model complexity that the model generalizes poorly to unseen data. \n",
    "    * $\\text{Var}[\\hat{f}(x)] = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]$\n",
    "    * Avg of all differences between the pred and true values squared. \n",
    "* Tradeoff is the while we can seek to minimize both, optimizing for one will push the other up. \n",
    "* Total Error   \n",
    "    * $E[(y - \\hat{f}(x))^2] = \\text{Bias}^2[\\hat{f}(x)] + \\text{Var}[\\hat{f}(x)] + \\sigma^2$\n",
    "    These components represent the error of our statistical learning method, including the irreducible error (variance of the error term)\n",
    "\n",
    "\n",
    "**Assessing Classification Models**\n",
    "* The error rate is used here to judge the learning method. \n",
    "    * $\\text{Error Rate} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}(y_{\\text{pred}i} \\neq y{\\text{true}_i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736fbff6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**The Bayes Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa878c77",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
