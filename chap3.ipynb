{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc5e131a",
   "metadata": {},
   "source": [
    "**Chapter 3 - Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49660ab2",
   "metadata": {},
   "source": [
    "Regression is the act of fitting a model to a dataset. We try to fit the model as closely to our data as possible. Close can be considered in different ways, however, generally we can try to optimize for the least squares. \n",
    "\n",
    "On averages, we want the smallest Residual Sum of Squares. Sum of squares between the predicted values and the Y true values: \n",
    "* $RSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "So in our regression we need to pick betas in $y = \\beta_0 + \\beta_1 x + \\epsilon$ which minimize the RSS. \n",
    "\n",
    "Our intercept term represent the value of Y if all other Xs are 0. \n",
    "\n",
    "*Law of Law Numbers* - As we take increasing samples, the sample statistic will begin to resemble the population statistic. \n",
    "*Central Limit Theorum* - As we continue taking sample means, the sampled means will begin to follow a normal distribution. \n",
    "\n",
    "When we fit a regression model, we have no guarantee that our coefficients will actually be the true coeffcients. However, if we conduct many regressions with multiple datasets and average the coefficients we can likely get close to the true coefficient values. \n",
    "\n",
    "The Standard Error tells how far away our sample mean is likely away from the true population mean. SE is simply the sample standard deviation divided by the square root of N. Intuitively the more N or samples we have the less standard error we'll have. More sample greater likelihood our sample mean is representative of the poplation mean. \n",
    "\n",
    "Similarly, we can compute the SE of our coefficient as well to gain a sense of whether or not they're representative of the population coefficient. \n",
    "* $SE(\\hat{\\beta}_1) = \\sqrt{\\frac{MSE}{\\sum(x_i - \\bar{x})^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500d3f7f",
   "metadata": {},
   "source": [
    "Standard Errors will also help us calculate confidence intervals. Confidence Intervals are ways of expressing a statistic with a margin of error. \n",
    "Confidence Interval = Significant Level * Standard Error\n",
    "Sig Level is determined by the confidence we want to have for 95 percent, 1.96\n",
    "Standard Error is the Population Standard Deviation / sqrt of N. \n",
    "Then 95% of the time the statistic should be represented without our window. \n",
    "\n",
    "*How to frame the utility of Confidence Intervals for regression?*\n",
    "* If we determine that for Beta1 which is advertising budget and our Y is total sales. If we estimate a confidence intevals for Beta1 of [5,10] then we could say that 1 extra dollar of advertising should lead to between 5 and 10 extra dollars in sales. \n",
    "\n",
    "*Purpose of coefficient values for Hypothesis testing?*\n",
    "* If we want to test whether or not some variable Beta1 has an influence on the outcome variable we could say that if Beta1 is 0 then the model is telling us there is no relationship. So how big of a coefficient would we need to determine that there is a relationship? One thing to consider is the standard error of the coefficient. A large standard error would suggest that we need a larger coefficient to be sure there is a relationship. A smaller SE would mean smaller coefficients are ok to show relationships. We can calulate the t-statistic, \n",
    "    * T = ( Beta1 - 0 ) / Standard Error\n",
    "The t statistic tells us how many standard deviations away the statistic is from the mean. Using the T statistic, we can then compute the Pvalue. \n",
    "The Pvalue tell us that given the H0 or null hypothesis is true. What are the chances that we would see data like this? If the pvalue is under 0.05 we reject as is seems Beta 1 does have an impact. \n",
    "\n",
    "* Type 1 Error = Alpha  = False Positive \n",
    "    * The risk we're willing to take of a false postive. Or in other words, rejective the null hypothesis when we shouldnt have. \n",
    "* Typer 2 Error = Beta = False Negative  \n",
    "    * Typically used to help determine power which is 1- beta, and we typically go for ROT 80% desired power. This means chances of a false negative are 20? \n",
    "\n",
    "Remember power is a signal of how strong your test is. More specifically, it is the likelihood of if there is a positive effect, what are the odds that your test will be able to detect such. Power is a function of\n",
    "\n",
    "    * Sample Size - the larger the sample size the greater our power, intuitive\n",
    "    * Effect Size - how large of an impact we're trying to see \n",
    "    * Significant Level - typically 0.05\n",
    "    * Population or sample variance - greater variance will reduce power\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680241e5",
   "metadata": {},
   "source": [
    "**Assessing the extent to which the model fits the data**\n",
    "\n",
    "Once we have determined which coefficients indeed have an impact on the Y outcome variable we want to quantify how well our model actually does. MSE can tell us something, but its not an intuitive metric to see how well does our model actually model Y? \n",
    "\n",
    "We can look at two unrelated quantities: \n",
    "* RSE - Residual Standard Error \n",
    "    * Even if we had perfect coefficients we still wouldnt be able to perfectly model Y.\n",
    "    * Essentially the standard deviation of our errors. It is known as a measure of the lack of fit. \n",
    "    * Expressed in actual terms of Y. \n",
    "    * So the output number here and whether or not the RSE is exceptable depends on the context of the problem. \n",
    "* $R^2$ \n",
    "    * How much of the variance of Y is our model able to explain. \n",
    "    * Unlike RSE which is expressed in unit of Y, the $R^2$  is able to provide a number between 0-1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0fec0",
   "metadata": {},
   "source": [
    "**Multiple Linear Regression**\n",
    "\n",
    "In the real world we likely have access to multiple predictors we could use to model Y. Instead of running n simple linear regressions we can run multiple linear regression to capture all of these predictors. This has several benefits over using multiple simple linear regression, as when we just have one predictor the coefficient ignore the influence of the other predictors on the outcome variable Y. The simpsons paradox also describes that some variables and their relationship with Y might become completely diminished after introducing other variables. We're also able to still model the influence of one coefficient while keeping others constant. \n",
    "\n",
    "To find the optimal coefficients, we select them using least squares or the coefficients that minimize the Square Sum of Residuals. \n",
    "\n",
    "The benefit is also that by introducing more predictors we can eliminate some of the chances of confounding variables. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692184ae",
   "metadata": {},
   "source": [
    "Questions we care about in multiple regression? \n",
    "\n",
    "1. Is at least one of the predictors useful in predicting out outcome variable? \n",
    "\n",
    "We essentially have to conduct a test to determine whether or not all parameters are 0. Quick refresh on the different types of test: \n",
    "z-test: when we want to test a single metric and the population variance is known (unlikely in real world)\n",
    "t-test: tests single parameter when the population variance is unknown (as population is unknown this one has heavier tails)\n",
    "f-tests: tests ratio of variances of multiple paramters\n",
    "\n",
    "The f-test in multiple regression can help us compare the variance explained by our model and the variance left over. A large f test statistic and a corresponding p-value less than 0.05 will lead us to reject the null hypothesis. \n",
    "\n",
    "2. Do all of them help or just a subset of predictors? \n",
    "\n",
    "Once we determine that our variable are useful, we need to conduct variable selection (or feature engineering in ML). Why? At scale having lots of features might become computationally expensive and we might lose interpretability. There different approached to conduct variable selection: \n",
    "\n",
    "Forward selection: We can do many simple linear regressions, between each predictor and outcome, then add the paramter to our model step wise starting with the parameter which had the lowest Pvalue. Continue adding until some stopping criteria is met like no more parameter with Pvalue less than 0.05. Or stop when error or loss stops improving. \n",
    "\n",
    "Backward Selection: The opposite, we start with all possible paramters and will continuously remove paramters from our model until a stopping criteria is met. \n",
    "\n",
    "Mixed Selection: This is like forward selection but we mix in backwards selection. Since Pvalues of the paramter can change once they are included in a model with other features. So we can add but every know an then reevaluate the pvalues of the parameters. \n",
    "\n",
    "3. How well does the model fit the data? \n",
    "\n",
    "For regression, the R2 metric is very insightful and can be used to compare models and their predictors. Some predictors might not lead to very signification jumps in R2. What's very interesting is that the R2 is essentially the squared correlation between our Y pred and the true Y. Squared because corr is -1 to 1. \n",
    "\n",
    "4. Given a subset of predictors, what is worth predicting? \n",
    "\n",
    "The issue is that we have to make an assumption that the model we're using is the one which best represent the real life model (which is most likely way more complex than a linear model).\n",
    "\n",
    "Remember we can calculate confidence intervals of our Parameters, similarly we can calculate prediciton intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8cce78",
   "metadata": {},
   "source": [
    "**How to deal with Qualitative Variables?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bbad00",
   "metadata": {},
   "source": [
    "For example, if trying to predict things like credit score we might encounter a dummy variable such as a binary owns a home or doesnt own a home variable. We could encode these using two different  \n",
    "\n",
    "* In 0/1 encoding the intercept would be the value the credit score without home owndership, the B1 would be the impact on credit score with home ownership. (Recommended)\n",
    "* You could also do -1 and 1, then intercept is the average credit score amongst homeowners and non homeowners, B1 represent half the difference between two groups. \n",
    "\n",
    "What if we have more than two variables? Perhaps what about if someone if form the North, South, East, or West? Here we could use 3 dummy variables with all 0/1 for North, South, East. If all of them are off we could assume the baseline is west. Then each parameter represents the different between West and each respective cardinal direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d1cab",
   "metadata": {},
   "source": [
    "**Extending our Linear Regression Concepts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52619bdd",
   "metadata": {},
   "source": [
    "So far two big assumptions of our linear models have been their additive nature and the linear form. \n",
    "\n",
    "One way which we can remove the additive assumption or add a way to capture non additive information is by the inclusion of an interaction term. In our current state, some paramters might be influenced by each other. For example, if we are trying to predict credit card balance using predictors of income and binary student, adding an interaction terms such as (income x Student) would allow us to model that someone's income probably has varying effect on they credit balance depending whether or not they're a student. \n",
    "\n",
    "The Second was is extending our regression to become polynomial regression. By adding polynomial terms to our model like income squared we can capture non linear relationships. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806096d2",
   "metadata": {},
   "source": [
    "**Common Issues during Regression**\n",
    "\n",
    "1. Non Linearity of the response predictor relationship. Not all data relationships will be linear. A simple way to test this is to plot the residuals for the fitted value. If there is a relationship of the points here then there should be no discernable relationship. Typically adding polynomial terms to the model function will help. \n",
    "\n",
    "2. Correlation of the error terms. Occure frequently in time-series dats when one prediction error sheds light on what the next error might be. What helps here is doing a log transformation of the outcome variable.  \n",
    "\n",
    "3. Variance of the Error terms. We'd like to hope that the variance of the error terms is constant across all ranges of Y (high and low Y values). You could apply log or tranform the Y outcome variable, or we could think about a WLS, weight regression which penalizes noisy data and observations. \n",
    "\n",
    "4. Outliers, occur when our predicted Y is unreasonably far away from the true Y. This could be due to either missing or incomplete data, or missing a predictor entirely, or it could be becuase the model isnt complex enough. To remove outliers is tricky, first you can visualize them in a residuals plot. One option is to use box plots, rule of thumb is to remove points which are greater than 1.5*IQR + Q3 or less than Q1-1.5*IQR. Or studentizes residuals.\n",
    "\n",
    "5. High Leverage Points. Essentially an outlier of our training data or an observation of X. Calculate an H statistic. \n",
    "\n",
    "6. Colinearity of the predictors. If two variables describe a lot about each other and move similarly, then its very difficult to seperate them in modeling to determine how much each one is associated with Y. To try to see colinearity we could of course look at a correlation matric and try to see which one have a high absolute value. However, there is something called multi colinearity which is relationships between three or more predictors which might not show within a corr matrix. What is preferred is to use the VIF (Variance inflation factor), helps tell us how important a variable really is or if its, being influenced by another.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a8e4e3",
   "metadata": {},
   "source": [
    "**Comparing Linear Regression to KNN**\n",
    "\n",
    "Size K Neighbors depends on the Bias VAriance tradeoff. More K, increases bias. Less K Increases Variance. Should we pick a linear model (parametric) or KNN (non-parametric) model? It depends on what the true form is of f(x), we can tell which one is better by looking at a plot of residuals, and the MSE. \n",
    "\n",
    "KNN does well when there are few predictors although the parametric regression offers a nice interpretability advantage. KNN may not perform well when there are many predictors due to the curse of dimensionality. When you have to speard one data across so many features there may be no nearby observation that helps provide good indicators. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd25893f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "**LAB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7c541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "pd.set_option('display.precision', 2)  # Show 2 decimal places\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c482142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d19803ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif \n",
    "from statsmodels.stats.anova import anova_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d53dd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS, \n",
    "                  summarize, \n",
    "                  poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b862942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'MS',\n",
       " 'Out',\n",
       " '_',\n",
       " '_7',\n",
       " '__',\n",
       " '___',\n",
       " '__builtin__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '__vsc_ipynb_file__',\n",
       " '_dh',\n",
       " '_i',\n",
       " '_i1',\n",
       " '_i2',\n",
       " '_i3',\n",
       " '_i4',\n",
       " '_i5',\n",
       " '_i6',\n",
       " '_i7',\n",
       " '_i8',\n",
       " '_ih',\n",
       " '_ii',\n",
       " '_iii',\n",
       " '_oh',\n",
       " 'anova_lm',\n",
       " 'exit',\n",
       " 'get_ipython',\n",
       " 'load_data',\n",
       " 'np',\n",
       " 'open',\n",
       " 'pd',\n",
       " 'poly',\n",
       " 'quit',\n",
       " 'sm',\n",
       " 'summarize',\n",
       " 'vif']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspecting namespace and objects, builtin has print etc. \n",
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d60658ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use the boston housing dataset,\n",
    "\n",
    "boston_df = load_data(\"Boston\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8f10954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax',\n",
       "       'ptratio', 'lstat', 'medv'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41062b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 13 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   crim     506 non-null    float64\n",
      " 1   zn       506 non-null    float64\n",
      " 2   indus    506 non-null    float64\n",
      " 3   chas     506 non-null    int64  \n",
      " 4   nox      506 non-null    float64\n",
      " 5   rm       506 non-null    float64\n",
      " 6   age      506 non-null    float64\n",
      " 7   dis      506 non-null    float64\n",
      " 8   rad      506 non-null    int64  \n",
      " 9   tax      506 non-null    int64  \n",
      " 10  ptratio  506 non-null    float64\n",
      " 11  lstat    506 non-null    float64\n",
      " 12  medv     506 non-null    float64\n",
      "dtypes: float64(10), int64(3)\n",
      "memory usage: 51.5 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.32e-03</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>6.58</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.09</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.73e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>6.42</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.97</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.73e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>7.18</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.97</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.24e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>7.00</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.06</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       crim    zn  indus  chas   nox    rm   age   dis  rad  tax  ptratio  \\\n",
       "0  6.32e-03  18.0   2.31     0  0.54  6.58  65.2  4.09    1  296     15.3   \n",
       "1  2.73e-02   0.0   7.07     0  0.47  6.42  78.9  4.97    2  242     17.8   \n",
       "2  2.73e-02   0.0   7.07     0  0.47  7.18  61.1  4.97    2  242     17.8   \n",
       "3  3.24e-02   0.0   2.18     0  0.46  7.00  45.8  6.06    3  222     18.7   \n",
       "\n",
       "   lstat  medv  \n",
       "0   4.98  24.0  \n",
       "1   9.14  21.6  \n",
       "2   4.03  34.7  \n",
       "3   2.94  33.4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.06e+02</td>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.61e+00</td>\n",
       "      <td>11.36</td>\n",
       "      <td>11.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.55</td>\n",
       "      <td>6.28</td>\n",
       "      <td>68.57</td>\n",
       "      <td>3.80</td>\n",
       "      <td>9.55</td>\n",
       "      <td>408.24</td>\n",
       "      <td>18.46</td>\n",
       "      <td>12.65</td>\n",
       "      <td>22.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.60e+00</td>\n",
       "      <td>23.32</td>\n",
       "      <td>6.86</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.70</td>\n",
       "      <td>28.15</td>\n",
       "      <td>2.11</td>\n",
       "      <td>8.71</td>\n",
       "      <td>168.54</td>\n",
       "      <td>2.16</td>\n",
       "      <td>7.14</td>\n",
       "      <td>9.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.32e-03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>3.56</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.00</td>\n",
       "      <td>187.00</td>\n",
       "      <td>12.60</td>\n",
       "      <td>1.73</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.20e-02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>5.89</td>\n",
       "      <td>45.02</td>\n",
       "      <td>2.10</td>\n",
       "      <td>4.00</td>\n",
       "      <td>279.00</td>\n",
       "      <td>17.40</td>\n",
       "      <td>6.95</td>\n",
       "      <td>17.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.57e-01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.54</td>\n",
       "      <td>6.21</td>\n",
       "      <td>77.50</td>\n",
       "      <td>3.21</td>\n",
       "      <td>5.00</td>\n",
       "      <td>330.00</td>\n",
       "      <td>19.05</td>\n",
       "      <td>11.36</td>\n",
       "      <td>21.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.68e+00</td>\n",
       "      <td>12.50</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>6.62</td>\n",
       "      <td>94.07</td>\n",
       "      <td>5.19</td>\n",
       "      <td>24.00</td>\n",
       "      <td>666.00</td>\n",
       "      <td>20.20</td>\n",
       "      <td>16.96</td>\n",
       "      <td>25.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.90e+01</td>\n",
       "      <td>100.00</td>\n",
       "      <td>27.74</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.87</td>\n",
       "      <td>8.78</td>\n",
       "      <td>100.00</td>\n",
       "      <td>12.13</td>\n",
       "      <td>24.00</td>\n",
       "      <td>711.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>37.97</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           crim      zn   indus    chas     nox      rm     age     dis  \\\n",
       "count  5.06e+02  506.00  506.00  506.00  506.00  506.00  506.00  506.00   \n",
       "mean   3.61e+00   11.36   11.14    0.07    0.55    6.28   68.57    3.80   \n",
       "std    8.60e+00   23.32    6.86    0.25    0.12    0.70   28.15    2.11   \n",
       "min    6.32e-03    0.00    0.46    0.00    0.39    3.56    2.90    1.13   \n",
       "25%    8.20e-02    0.00    5.19    0.00    0.45    5.89   45.02    2.10   \n",
       "50%    2.57e-01    0.00    9.69    0.00    0.54    6.21   77.50    3.21   \n",
       "75%    3.68e+00   12.50   18.10    0.00    0.62    6.62   94.07    5.19   \n",
       "max    8.90e+01  100.00   27.74    1.00    0.87    8.78  100.00   12.13   \n",
       "\n",
       "          rad     tax  ptratio   lstat    medv  \n",
       "count  506.00  506.00   506.00  506.00  506.00  \n",
       "mean     9.55  408.24    18.46   12.65   22.53  \n",
       "std      8.71  168.54     2.16    7.14    9.20  \n",
       "min      1.00  187.00    12.60    1.73    5.00  \n",
       "25%      4.00  279.00    17.40    6.95   17.02  \n",
       "50%      5.00  330.00    19.05   11.36   21.20  \n",
       "75%     24.00  666.00    20.20   16.96   25.00  \n",
       "max     24.00  711.00    22.00   37.97   50.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(boston_df.info())\n",
    "\n",
    "display(boston_df.head(4))\n",
    "\n",
    "display(boston_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e0178",
   "metadata": {},
   "source": [
    "Our targer will be medv, which is the median home value in the $000's. \n",
    "Out main predictor will first be LSTAT, which is % lower status of the population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a56a4261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intercept</th>\n",
       "      <th>lstat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   intercept  lstat\n",
       "0        1.0   4.98\n",
       "1        1.0   9.14\n",
       "2        1.0   4.03\n",
       "3        1.0   2.94\n",
       "4        1.0   5.33"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_x_matrix = pd.DataFrame({ 'intercept' : np.ones(boston_df.shape[0])   ,\n",
    "                                 'lstat' : boston_df['lstat']})\n",
    "\n",
    "# check to make sure it worked \n",
    "initial_x_matrix.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34b2cdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model and train it using fit\n",
    "y = boston_df['medv']\n",
    "model = sm.OLS(y,initial_x_matrix)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b5fd9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>t</th>\n",
       "      <th>P&gt;|t|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>34.55</td>\n",
       "      <td>0.56</td>\n",
       "      <td>61.41</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstat</th>\n",
       "      <td>-0.95</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-24.53</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            coef  std err      t  P>|t|\n",
       "intercept  34.55     0.56  61.41    0.0\n",
       "lstat      -0.95     0.04 -24.53    0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcccdfbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>medv</td>       <th>  R-squared:         </th> <td>   0.544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.543</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   601.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 31 Aug 2025</td> <th>  Prob (F-statistic):</th> <td>5.08e-88</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:45:47</td>     <th>  Log-Likelihood:    </th> <td> -1641.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3287.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   504</td>      <th>  BIC:               </th> <td>   3295.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   34.5538</td> <td>    0.563</td> <td>   61.415</td> <td> 0.000</td> <td>   33.448</td> <td>   35.659</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lstat</th>     <td>   -0.9500</td> <td>    0.039</td> <td>  -24.528</td> <td> 0.000</td> <td>   -1.026</td> <td>   -0.874</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>137.043</td> <th>  Durbin-Watson:     </th> <td>   0.892</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 291.373</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.453</td>  <th>  Prob(JB):          </th> <td>5.36e-64</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.319</td>  <th>  Cond. No.          </th> <td>    29.7</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       medv       & \\textbf{  R-squared:         } &     0.544   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.543   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     601.6   \\\\\n",
       "\\textbf{Date:}             & Sun, 31 Aug 2025 & \\textbf{  Prob (F-statistic):} &  5.08e-88   \\\\\n",
       "\\textbf{Time:}             &     15:45:47     & \\textbf{  Log-Likelihood:    } &   -1641.5   \\\\\n",
       "\\textbf{No. Observations:} &         506      & \\textbf{  AIC:               } &     3287.   \\\\\n",
       "\\textbf{Df Residuals:}     &         504      & \\textbf{  BIC:               } &     3295.   \\\\\n",
       "\\textbf{Df Model:}         &           1      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{intercept} &      34.5538  &        0.563     &    61.415  &         0.000        &       33.448    &       35.659     \\\\\n",
       "\\textbf{lstat}     &      -0.9500  &        0.039     &   -24.528  &         0.000        &       -1.026    &       -0.874     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 137.043 & \\textbf{  Durbin-Watson:     } &    0.892  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } &  291.373  \\\\\n",
       "\\textbf{Skew:}          &   1.453 & \\textbf{  Prob(JB):          } & 5.36e-64  \\\\\n",
       "\\textbf{Kurtosis:}      &   5.319 & \\textbf{  Cond. No.          } &     29.7  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   medv   R-squared:                       0.544\n",
       "Model:                            OLS   Adj. R-squared:                  0.543\n",
       "Method:                 Least Squares   F-statistic:                     601.6\n",
       "Date:                Sun, 31 Aug 2025   Prob (F-statistic):           5.08e-88\n",
       "Time:                        15:45:47   Log-Likelihood:                -1641.5\n",
       "No. Observations:                 506   AIC:                             3287.\n",
       "Df Residuals:                     504   BIC:                             3295.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     34.5538      0.563     61.415      0.000      33.448      35.659\n",
       "lstat         -0.9500      0.039    -24.528      0.000      -1.026      -0.874\n",
       "==============================================================================\n",
       "Omnibus:                      137.043   Durbin-Watson:                   0.892\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              291.373\n",
       "Skew:                           1.453   Prob(JB):                     5.36e-64\n",
       "Kurtosis:                       5.319   Cond. No.                         29.7\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "984d2fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a plotting functions \n",
    "\n",
    "def abline (ax, b, m,*args , **kwargs): \n",
    "    \"Add a line with slop m and intercept B to ax\"\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = [m  * xlim[0] + b, m * xlim[1] + b]\n",
    "    ax.plot(xlim, ylim, *args , **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd8a167",
   "metadata": {},
   "source": [
    "**Multiple Linear Regression**\n",
    "lets train a model on all predictors available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97097d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crim',\n",
       " 'zn',\n",
       " 'indus',\n",
       " 'chas',\n",
       " 'nox',\n",
       " 'rm',\n",
       " 'age',\n",
       " 'dis',\n",
       " 'rad',\n",
       " 'tax',\n",
       " 'ptratio',\n",
       " 'lstat']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19e1f8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>t</th>\n",
       "      <th>P&gt;|t|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>4.16e+01</td>\n",
       "      <td>4.94e+00</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0.00e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crim</th>\n",
       "      <td>-1.21e-01</td>\n",
       "      <td>3.30e-02</td>\n",
       "      <td>-3.68</td>\n",
       "      <td>0.00e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zn</th>\n",
       "      <td>4.70e-02</td>\n",
       "      <td>1.40e-02</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.00e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indus</th>\n",
       "      <td>1.35e-02</td>\n",
       "      <td>6.20e-02</td>\n",
       "      <td>0.22</td>\n",
       "      <td>8.29e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chas</th>\n",
       "      <td>2.84e+00</td>\n",
       "      <td>8.70e-01</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1.00e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nox</th>\n",
       "      <td>-1.88e+01</td>\n",
       "      <td>3.85e+00</td>\n",
       "      <td>-4.87</td>\n",
       "      <td>0.00e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rm</th>\n",
       "      <td>3.66e+00</td>\n",
       "      <td>4.20e-01</td>\n",
       "      <td>8.71</td>\n",
       "      <td>0.00e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>3.60e-03</td>\n",
       "      <td>1.30e-02</td>\n",
       "      <td>0.27</td>\n",
       "      <td>7.87e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis</th>\n",
       "      <td>-1.49e+00</td>\n",
       "      <td>2.02e-01</td>\n",
       "      <td>-7.39</td>\n",
       "      <td>0.00e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rad</th>\n",
       "      <td>2.89e-01</td>\n",
       "      <td>6.70e-02</td>\n",
       "      <td>4.33</td>\n",
       "      <td>0.00e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tax</th>\n",
       "      <td>-1.27e-02</td>\n",
       "      <td>4.00e-03</td>\n",
       "      <td>-3.34</td>\n",
       "      <td>1.00e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptratio</th>\n",
       "      <td>-9.38e-01</td>\n",
       "      <td>1.32e-01</td>\n",
       "      <td>-7.09</td>\n",
       "      <td>0.00e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstat</th>\n",
       "      <td>-5.52e-01</td>\n",
       "      <td>5.10e-02</td>\n",
       "      <td>-10.90</td>\n",
       "      <td>0.00e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               coef   std err      t     P>|t|\n",
       "intercept  4.16e+01  4.94e+00   8.43  0.00e+00\n",
       "crim      -1.21e-01  3.30e-02  -3.68  0.00e+00\n",
       "zn         4.70e-02  1.40e-02   3.38  1.00e-03\n",
       "indus      1.35e-02  6.20e-02   0.22  8.29e-01\n",
       "chas       2.84e+00  8.70e-01   3.26  1.00e-03\n",
       "nox       -1.88e+01  3.85e+00  -4.87  0.00e+00\n",
       "rm         3.66e+00  4.20e-01   8.71  0.00e+00\n",
       "age        3.60e-03  1.30e-02   0.27  7.87e-01\n",
       "dis       -1.49e+00  2.02e-01  -7.39  0.00e+00\n",
       "rad        2.89e-01  6.70e-02   4.33  0.00e+00\n",
       "tax       -1.27e-02  4.00e-03  -3.34  1.00e-03\n",
       "ptratio   -9.38e-01  1.32e-01  -7.09  0.00e+00\n",
       "lstat     -5.52e-01  5.10e-02 -10.90  0.00e+00"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make X equal to all columns other than our predictor \n",
    "Y = boston_df['medv']\n",
    "X = MS(list(boston_df.loc[:, boston_df.columns != 'medv'].columns)).fit_transform(boston_df)\n",
    "\n",
    "model = sm.OLS(Y, X)\n",
    "\n",
    "results = model.fit()\n",
    "\n",
    "summarize(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649c5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>crim</th>\n",
       "      <td>1.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zn</th>\n",
       "      <td>2.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indus</th>\n",
       "      <td>3.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chas</th>\n",
       "      <td>1.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nox</th>\n",
       "      <td>4.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rm</th>\n",
       "      <td>1.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>3.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis</th>\n",
       "      <td>3.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rad</th>\n",
       "      <td>7.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tax</th>\n",
       "      <td>9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptratio</th>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstat</th>\n",
       "      <td>2.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          vif\n",
       "crim     1.77\n",
       "zn       2.30\n",
       "indus    3.99\n",
       "chas     1.07\n",
       "nox      4.37\n",
       "rm       1.91\n",
       "age      3.09\n",
       "dis      3.95\n",
       "rad      7.45\n",
       "tax      9.00\n",
       "ptratio  1.80\n",
       "lstat    2.87"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a variance inflation factor DF. \n",
    "vif_values = [vif(X,i)\n",
    "             for i in range(1,X.shape[1])]\n",
    "vif_dataframe = pd.DataFrame({'vif' : vif_values}, \n",
    "                             index= X.columns[1:]\n",
    "                             )\n",
    "\n",
    "vif_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dff0ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create interaction terms we could go like this \n",
    "\n",
    "X = MS(['lstat', \n",
    "        'age',\n",
    "        ('lstat', 'age')]).fit_transform(boston_df)\n",
    "\n",
    "# to create polynomial terms and introduce non linearity we could \n",
    "\n",
    "X = MS([poly('lstat', degree=2), \n",
    "        'age']).fit_transform(boston_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6d930",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "\n",
    "Q1) the pvalues of the coefficients for a model correspond to the p value that where our null hypothesis is that the parameter = 0, that there is no relationship. In table 3.4, all parameter are highly significant where we'd reject the null hypothesis. One of the parameters however has a high pvalue of 0.85, we fail to reject the null hypothesis. Very important to not say that this means there is no relationship with the outcome variable. All it can help us conclude is that this study did no procide sufficient evidence for us to reject the null hypothesis. What could have gone wrong? \n",
    "* Insufficient statistical power - what was the power of the test? What was our training data size? \n",
    "* Small effect size, maybe the effect the variable has on the Y is very small.  \n",
    "\n",
    "Better than looking at the pvalue for a coefficient is to look at the confidence interval of it. A pvalue cannot be significant if the confidence interval includes 0. \n",
    "\n",
    "Q2) difference between KNN Classifer and KNN Regression? Similar algorithm by looking at the closest K neighbors according to some distance metric. We then take the majority class or average value for regression. KNN is non parametric and doesnt assume a form. Knn when k is small has high variance, when K is large, large bias. Also very susceptible to the curse of dimensionality. Another point is that for training and for inference you essentially need to have all data in memory. \n",
    "\n",
    "Q3) For a dataset where the linear model is the true fit, will a linear model perform better than a cubic regression model? No, the cubic will always perform better than the linear model because it can be linear and non linear but only for the training data! It may well be that the cubic data is overfit and doesnt generalize well. \n",
    "\n",
    "Q4) When we regress X onto Y, we say the R2 is the square of the correlation between X and Y, how? R² measures how much of Y's variation is explained by the linear relationship with X, squaring the correlation gives us the proportion of variance explained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70365258",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Applied Q8) \n",
    "Perform a simple regression of MPG using only Horsepower. \n",
    "\n",
    "i.  Is there a relationship between the predictor and the response? \n",
    "ii. How strong is the relationship between the predictor and the response? \n",
    "iii. Is it a positive or negative relationship?  \n",
    "iv. What is the predicted MPG with a 98 horsepower? (what are the confidence and prediction intervals?)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee0aa3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chevrolet chevelle malibu</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buick skylark 320</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plymouth satellite</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amc rebel sst</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ford torino</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            mpg  cylinders  displacement  horsepower  weight  \\\n",
       "name                                                                           \n",
       "chevrolet chevelle malibu  18.0          8         307.0         130    3504   \n",
       "buick skylark 320          15.0          8         350.0         165    3693   \n",
       "plymouth satellite         18.0          8         318.0         150    3436   \n",
       "amc rebel sst              16.0          8         304.0         150    3433   \n",
       "ford torino                17.0          8         302.0         140    3449   \n",
       "\n",
       "                           acceleration  year  origin  \n",
       "name                                                   \n",
       "chevrolet chevelle malibu          12.0    70       1  \n",
       "buick skylark 320                  11.5    70       1  \n",
       "plymouth satellite                 11.0    70       1  \n",
       "amc rebel sst                      12.0    70       1  \n",
       "ford torino                        10.5    70       1  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_df = load_data(\"Auto\")\n",
    "auto_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c779a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our predixtors\n",
    "q8_x = pd.DataFrame({ 'intercept' : np.ones(auto_df.shape[0])   ,\n",
    "                                 'horsepower' : auto_df['horsepower']})\n",
    "# creat Y variable\n",
    "q8_y = auto_df['mpg']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce07fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
