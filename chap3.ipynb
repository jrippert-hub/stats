{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc5e131a",
   "metadata": {},
   "source": [
    "**Chapter 3 - Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49660ab2",
   "metadata": {},
   "source": [
    "Regression is the act of fitting a model to a dataset. We try to fit the model as closely to our data as possible. Close can be considered in different ways, however, generally we can try to optimize for the least squares. \n",
    "\n",
    "On averages, we want the smallest Residual Sum of Squares. Sum of squares between the predicted values and the Y true values: \n",
    "* $RSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "So in our regression we need to pick betas in $y = \\beta_0 + \\beta_1 x + \\epsilon$ which minimize the RSS. \n",
    "\n",
    "Our intercept term represent the value of Y if all other Xs are 0. \n",
    "\n",
    "*Law of Law Numbers* - As we take increasing samples, the sample statistic will begin to resemble the population statistic. \n",
    "*Central Limit Theorum* - As we continue taking sample means, the sampled means will begin to follow a normal distribution. \n",
    "\n",
    "When we fit a regression model, we have no guarantee that our coefficients will actually be the true coeffcients. However, if we conduct many regressions with multiple datasets and average the coefficients we can likely get close to the true coefficient values. \n",
    "\n",
    "The Standard Error tells how far away our sample mean is likely away from the true population mean. SE is simply the sample standard deviation divided by the square root of N. Intuitively the more N or samples we have the less standard error we'll have. More sample greater likelihood our sample mean is representative of the poplation mean. \n",
    "\n",
    "Similarly, we can compute the SE of our coefficient as well to gain a sense of whether or not they're representative of the population coefficient. \n",
    "* $SE(\\hat{\\beta}_1) = \\sqrt{\\frac{MSE}{\\sum(x_i - \\bar{x})^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500d3f7f",
   "metadata": {},
   "source": [
    "Standard Errors will also help us calculate confidence intervals. Confidence Intervals are ways of expressing a statistic with a margin of error. \n",
    "Confidence Interval = Significant Level * Standard Error\n",
    "Sig Level is determined by the confidence we want to have for 95 percent, 1.96\n",
    "Standard Error is the Population Standard Deviation / sqrt of N. \n",
    "Then 95% of the time the statistic should be represented without our window. \n",
    "\n",
    "*How to frame the utility of Confidence Intervals for regression?*\n",
    "* If we determine that for Beta1 which is advertising budget and our Y is total sales. If we estimate a confidence intevals for Beta1 of [5,10] then we could say that 1 extra dollar of advertising should lead to between 5 and 10 extra dollars in sales. \n",
    "\n",
    "*Purpose of coefficient values for Hypothesis testing?*\n",
    "* If we want to test whether or not some variable Beta1 has an influence on the outcome variable we could say that if Beta1 is 0 then the model is telling us there is no relationship. So how big of a coefficient would we need to determine that there is a relationship? One thing to consider is the standard error of the coefficient. A large standard error would suggest that we need a larger coefficient to be sure there is a relationship. A smaller SE would mean smaller coefficients are ok to show relationships. We can calulate the t-statistic, \n",
    "    * T = ( Beta1 - 0 ) / Standard Error\n",
    "The t statistic tells us how many standard deviations away the statistic is from the mean. Using the T statistic, we can then compute the Pvalue. \n",
    "The Pvalue tell us that given the H0 or null hypothesis is true. What are the chances that we would see data like this? If the pvalue is under 0.05 we reject as is seems Beta 1 does have an impact. \n",
    "\n",
    "* Type 1 Error = Alpha  = False Positive \n",
    "    * The risk we're willing to take of a false postive. Or in other words, rejective the null hypothesis when we shouldnt have. \n",
    "* Typer 2 Error = Beta = False Negative  \n",
    "    * Typically used to help determine power which is 1- beta, and we typically go for ROT 80% desired power. This means chances of a false negative are 20? \n",
    "\n",
    "Remember power is a signal of how strong your test is. More specifically, it is the likelihood of if there is a positive effect, what are the odds that your test will be able to detect such. Power is a function of\n",
    "\n",
    "    * Sample Size - the larger the sample size the greater our power, intuitive\n",
    "    * Effect Size - how large of an impact we're trying to see \n",
    "    * Significant Level - typically 0.05\n",
    "    * Population or sample variance - greater variance will reduce power\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680241e5",
   "metadata": {},
   "source": [
    "**Assessing the extent to which the model fits the data**\n",
    "\n",
    "Once we have determined which coefficients indeed have an impact on the Y outcome variable we want to quantify how well our model actually does. MSE can tell us something, but its not an intuitive metric to see how well does our model actually model Y? \n",
    "\n",
    "We can look at two unrelated quantities: \n",
    "* RSE - Residual Standard Error \n",
    "    * Even if we had perfect coefficients we still wouldnt be able to perfectly model Y.\n",
    "    * Essentially the standard deviation of our errors. It is known as a measure of the lack of fit. \n",
    "    * Expressed in actual terms of Y. \n",
    "    * So the output number here and whether or not the RSE is exceptable depends on the context of the problem. \n",
    "* $R^2$ \n",
    "    * How much of the variance of Y is our model able to explain. \n",
    "    * Unlike RSE which is expressed in unit of Y, the $R^2$  is able to provide a number between 0-1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0fec0",
   "metadata": {},
   "source": [
    "**Multiple Linear Regression**\n",
    "\n",
    "In the real world we likely have access to multiple predictors we could use to model Y. Instead of running n simple linear regressions we can run multiple linear regression to capture all of these predictors. This has several benefits over using multiple simple linear regression, as when we just have one predictor the coefficient ignore the influence of the other predictors on the outcome variable Y. The simpsons paradox also describes that some variables and their relationship with Y might become completely diminished after introducing other variables. We're also able to still model the influence of one coefficient while keeping others constant. \n",
    "\n",
    "To find the optimal coefficients, we select them using least squares or the coefficients that minimize the Square Sum of Residuals. \n",
    "\n",
    "The benefit is also that by introducing more predictors we can eliminate some of the chances of confounding variables. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692184ae",
   "metadata": {},
   "source": [
    "Questions we care about in multiple regression? \n",
    "\n",
    "1. Is at least one of the predictors useful in predicting out outcome variable? \n",
    "\n",
    "We essentially have to conduct a test to determine whether or not all parameters are 0. Quick refresh on the different types of test: \n",
    "z-test: when we want to test a single metric and the population variance is known (unlikely in real world)\n",
    "t-test: tests single parameter when the population variance is unknown (as population is unknown this one has heavier tails)\n",
    "f-tests: tests ratio of variances of multiple paramters\n",
    "\n",
    "The f-test in multiple regression can help us compare the variance explained by our model and the variance left over. A large f test statistic and a corresponding p-value less than 0.05 will lead us to reject the null hypothesis. \n",
    "\n",
    "2. Do all of them help or just a subset of predictors? \n",
    "\n",
    "Once we determine that our variable are useful, we need to conduct variable selection (or feature engineering in ML). Why? At scale having lots of features might become computationally expensive and we might lose interpretability. There different approached to conduct variable selection: \n",
    "\n",
    "Forward selection: We can do many simple linear regressions, between each predictor and outcome, then add the paramter to our model step wise starting with the parameter which had the lowest Pvalue. Continue adding until some stopping criteria is met like no more parameter with Pvalue less than 0.05. Or stop when error or loss stops improving. \n",
    "\n",
    "Backward Selection: The opposite, we start with all possible paramters and will continuously remove paramters from our model until a stopping criteria is met. \n",
    "\n",
    "Mixed Selection: This is like forward selection but we mix in backwards selection. Since Pvalues of the paramter can change once they are included in a model with other features. So we can add but every know an then reevaluate the pvalues of the parameters. \n",
    "\n",
    "3. How well does the model fit the data? \n",
    "\n",
    "For regression, the R2 metric is very insightful and can be used to compare models and their predictors. Some predictors might not lead to very signification jumps in R2. What's very interesting is that the R2 is essentially the squared correlation between our Y pred and the true Y. Squared because corr is -1 to 1. \n",
    "\n",
    "4. Given a subset of predictors, what is worth predicting? \n",
    "\n",
    "The issue is that we have to make an assumption that the model we're using is the one which best represent the real life model (which is most likely way more complex than a linear model).\n",
    "\n",
    "Remember we can calculate confidence intervals of our Parameters, similarly we can calculate prediciton intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8cce78",
   "metadata": {},
   "source": [
    "**How to deal with Qualitative Variables?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bbad00",
   "metadata": {},
   "source": [
    "For example, if trying to predict things like credit score we might encounter a dummy variable such as a binary owns a home or doesnt own a home variable. We could encode these using two different  \n",
    "\n",
    "* In 0/1 encoding the intercept would be the value the credit score without home owndership, the B1 would be the impact on credit score with home ownership. (Recommended)\n",
    "* You could also do -1 and 1, then intercept is the average credit score amongst homeowners and non homeowners, B1 represent half the difference between two groups. \n",
    "\n",
    "What if we have more than two variables? Perhaps what about if someone if form the North, South, East, or West? Here we could use 3 dummy variables with all 0/1 for North, South, East. If all of them are off we could assume the baseline is west. Then each parameter represents the different between West and each respective cardinal direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d1cab",
   "metadata": {},
   "source": [
    "**Extending our Linear Regression Concepts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52619bdd",
   "metadata": {},
   "source": [
    "So far two big assumptions of our linear models have been their additive nature and the linear form. \n",
    "\n",
    "One way which we can remove the additive assumption or add a way to capture non additive information is by the inclusion of an interaction term. In our current state, some paramters might be influenced by each other. For example, if we are trying to predict credit card balance using predictors of income and binary student, adding an interaction terms such as (income x Student) would allow us to model that someone's income probably has varying effect on they credit balance depending whether or not they're a student. \n",
    "\n",
    "The Second was is extending our regression to become polynomial regression. By adding polynomial terms to our model like income squared we can capture non linear relationships. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806096d2",
   "metadata": {},
   "source": [
    "**Common Issues during Regression**\n",
    "\n",
    "1. Non Linearity of the response predictor relationship. Not all data relationships will be linear. A simple way to test this is to plot the residuals for the fitted value. If there is a relationship of the points here then there should be no discernable relationship. Typically adding polynomial terms to the model function will help. \n",
    "\n",
    "2. Correlation of the error terms. Occure frequently in time-series dats when one prediction error sheds light on what the next error might be. What helps here is doing a log transformation of the outcome variable.  \n",
    "\n",
    "3. Variance of the Error terms. We'd like to hope that the variance of the error terms is constant across all ranges of Y (high and low Y values). You could apply log or tranform the Y outcome variable, or we could think about a WLS, weight regression which penalizes noisy data and observations. \n",
    "\n",
    "4. Outliers, occur when our predicted Y is unreasonably far away from the true Y. This could be due to either missing or incomplete data, or missing a predictor entirely, or it could be becuase the model isnt complex enough. To remove outliers is tricky, first you can visualize them in a residuals plot. One option is to use box plots, rule of thumb is to remove points which are greater than 1.5*IQR + Q3 or less than Q1-1.5*IQR. Or studentizes residuals.\n",
    "\n",
    "5. High Leverage Points. Essentially an outlier of our training data or an observation of X. Calculate an H statistic. \n",
    "\n",
    "6. Colinearity of the predictors. If two variables describe a lot about each other and move similarly, then its very difficult to seperate them in modeling to determine how much each one is associated with Y. To try to see colinearity we could of course look at a correlation matric and try to see which one have a high absolute value. However, there is something called multi colinearity which is relationships between three or more predictors which might not show within a corr matrix. What is preferred is to use the VIF (Variance inflation factor), helps tell us how important a variable really is or if its, being influenced by another.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a8e4e3",
   "metadata": {},
   "source": [
    "**Comparing Linear Regression to KNN**\n",
    "\n",
    "Size K Neighbors depends on the Bias VAriance tradeoff. More K, increases bias. Less K Increases Variance. Should we pick a linear model (parametric) or KNN (non-parametric) model? It depends on what the true form is of f(x), we can tell which one is better by looking at a plot of residuals, and the MSE. \n",
    "\n",
    "KNN does well when there are few predictors although the parametric regression offers a nice interpretability advantage. KNN may not perform well when there are many predictors due to the curse of dimensionality. When you have to speard one data across so many features there may be no nearby observation that helps provide good indicators. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1344b04a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
